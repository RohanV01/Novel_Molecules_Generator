{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/user/DD_Pipeline/env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-08-30 12:52:14.726685: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-08-30 12:52:16.921965: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-08-30 12:52:17.716347: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-08-30 12:52:17.931484: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-08-30 12:52:19.453492: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-30 12:52:27.421075: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/mnt/c/Users/user/DD_Pipeline/env/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: /mnt/c/Users/user/DD_Pipeline/Training_SMILES/chunk1.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/user/DD_Pipeline/env/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='20331' max='20331' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [20331/20331 2:00:19, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.102100</td>\n",
       "      <td>0.092291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.093500</td>\n",
       "      <td>0.081216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.086700</td>\n",
       "      <td>0.077186</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed on file /mnt/c/Users/user/DD_Pipeline/Training_SMILES/chunk1.csv. Checkpoint saved at ./smiles_gpt2/checkpoint-1.\n",
      "Waiting for 5 minutes before training on the next chunk...\n",
      "Processing file: /mnt/c/Users/user/DD_Pipeline/Training_SMILES/chunk10.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 62616 examples [00:00, 457687.22 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 62616/62616 [00:05<00:00, 11737.23 examples/s]\n",
      "/mnt/c/Users/user/DD_Pipeline/env/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "There were missing keys in the checkpoint model loaded: ['lm_head.weight'].\n",
      "/mnt/c/Users/user/DD_Pipeline/env/lib/python3.10/site-packages/transformers/trainer.py:3098: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(os.path.join(checkpoint, OPTIMIZER_NAME), map_location=map_location)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='20331' max='19959' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [20331/19959 : < :, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed on file /mnt/c/Users/user/DD_Pipeline/Training_SMILES/chunk10.csv. Checkpoint saved at ./smiles_gpt2/checkpoint-2.\n",
      "Waiting for 5 minutes before training on the next chunk...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 78\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining completed on file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Checkpoint saved at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcheckpoint_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWaiting for 5 minutes before training on the next chunk...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 78\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m300\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# 5 minutes\u001b[39;00m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining completed on all files!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Directory containing your CSV files\n",
    "data_dir = \"/mnt/c/Users/user/DD_Pipeline/Training_SMILES\"\n",
    "\n",
    "# Ensure the log directory exists\n",
    "log_dir = './training_logs'\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Set the eos_token as padding token\n",
    "\n",
    "# Load the model\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "def load_and_tokenize_chunk(file_path):\n",
    "    dataset = load_dataset('csv', data_files=file_path)\n",
    "\n",
    "    def tokenize_function(examples):\n",
    "        tokenized = tokenizer(examples['SMILES'], truncation=True, padding='max_length', max_length=300)\n",
    "        tokenized['labels'] = tokenized['input_ids'].copy()  # Set labels for the LM\n",
    "        return tokenized\n",
    "\n",
    "    tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "    split_dataset = tokenized_dataset['train'].train_test_split(test_size=0.15)\n",
    "    return split_dataset\n",
    "\n",
    "def log_losses(log_file_path, train_loss, val_loss=None):\n",
    "    with open(log_file_path, 'a') as log_file:\n",
    "        log_file.write(f\"Training loss: {train_loss}\\n\")\n",
    "        if val_loss:\n",
    "            log_file.write(f\"Validation loss: {val_loss}\\n\")\n",
    "\n",
    "csv_files = sorted([os.path.join(data_dir, f) for f in os.listdir(data_dir) if f.endswith('.csv')])\n",
    "\n",
    "for i, file_path in enumerate(csv_files):\n",
    "    print(f\"Processing file: {file_path}\")\n",
    "\n",
    "    split_dataset = load_and_tokenize_chunk(file_path)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./smiles_gpt2\",\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        learning_rate=5e-5,\n",
    "        weight_decay=0.01,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        num_train_epochs=3,\n",
    "        save_total_limit=2,\n",
    "        resume_from_checkpoint=True if i > 0 else None,\n",
    "        logging_dir=\"./logs\",\n",
    "        logging_steps=10,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=split_dataset['train'],\n",
    "        eval_dataset=split_dataset['test'],\n",
    "    )\n",
    "\n",
    "    train_output = trainer.train(resume_from_checkpoint=True if i > 0 else None)\n",
    "    checkpoint_dir = f'./smiles_gpt2/checkpoint-{i+1}'\n",
    "    model.save_pretrained(checkpoint_dir)\n",
    "    tokenizer.save_pretrained(checkpoint_dir)\n",
    "\n",
    "    train_loss = train_output.training_loss\n",
    "    log_file_path = f'training_logs/chunk_{i+1}_training_log.txt'\n",
    "    log_losses(log_file_path, train_loss)\n",
    "\n",
    "    print(f\"Training completed on file {file_path}. Checkpoint saved at {checkpoint_dir}.\")\n",
    "    print(\"Waiting for 5 minutes before training on the next chunk...\")\n",
    "    time.sleep(300)  # 5 minutes\n",
    "\n",
    "print(\"Training completed on all files!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/user/DD_Pipeline/env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-09-07 16:56:19.503825: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-09-07 16:56:22.078055: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-09-07 16:56:22.921311: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-09-07 16:56:23.143921: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-09-07 16:56:24.894392: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-07 16:56:33.229741: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: /mnt/c/Users/user/DD_Pipeline/Training_SMILES/chunk1.csv\n",
      "Error finding latest checkpoint: [Errno 2] No such file or directory: '/mnt/c/Users/user/DD_Pipeline/model_training/smiles_gpt2'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13554' max='13554' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13554/13554 2:06:49, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.058300</td>\n",
       "      <td>0.056311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.056400</td>\n",
       "      <td>0.050634</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed on file /mnt/c/Users/user/DD_Pipeline/Training_SMILES/chunk1.csv. Checkpoint saved at /mnt/c/Users/user/DD_Pipeline/model_training/smiles_gpt2/checkpoint-1.\n",
      "Waiting for 30 minutes before training on the next chunk...\n",
      "Processing file: /mnt/c/Users/user/DD_Pipeline/Training_SMILES/chunk2.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 82334/82334 [00:08<00:00, 9986.70 examples/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest checkpoint found: /mnt/c/Users/user/DD_Pipeline/model_training/smiles_gpt2/checkpoint-1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17496' max='17496' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [17496/17496 2:40:22, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.050100</td>\n",
       "      <td>0.046646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.047500</td>\n",
       "      <td>0.043779</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed on file /mnt/c/Users/user/DD_Pipeline/Training_SMILES/chunk2.csv. Checkpoint saved at /mnt/c/Users/user/DD_Pipeline/model_training/smiles_gpt2/checkpoint-2.\n",
      "Waiting for 30 minutes before training on the next chunk...\n",
      "Processing file: /mnt/c/Users/user/DD_Pipeline/Training_SMILES/chunk3.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 81814/81814 [00:08<00:00, 10176.26 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest checkpoint found: /mnt/c/Users/user/DD_Pipeline/model_training/smiles_gpt2/checkpoint-2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17386' max='17386' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [17386/17386 2:39:00, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.046500</td>\n",
       "      <td>0.043369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.045100</td>\n",
       "      <td>0.041348</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed on file /mnt/c/Users/user/DD_Pipeline/Training_SMILES/chunk3.csv. Checkpoint saved at /mnt/c/Users/user/DD_Pipeline/model_training/smiles_gpt2/checkpoint-3.\n",
      "Waiting for 30 minutes before training on the next chunk...\n",
      "Processing file: /mnt/c/Users/user/DD_Pipeline/Training_SMILES/chunk4.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 81600/81600 [00:08<00:00, 10053.19 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest checkpoint found: /mnt/c/Users/user/DD_Pipeline/model_training/smiles_gpt2/checkpoint-3\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17340' max='17340' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [17340/17340 2:38:50, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.043200</td>\n",
       "      <td>0.040992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.044400</td>\n",
       "      <td>0.039241</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed on file /mnt/c/Users/user/DD_Pipeline/Training_SMILES/chunk4.csv. Checkpoint saved at /mnt/c/Users/user/DD_Pipeline/model_training/smiles_gpt2/checkpoint-4.\n",
      "Waiting for 30 minutes before training on the next chunk...\n",
      "Processing file: /mnt/c/Users/user/DD_Pipeline/Training_SMILES/chunk5.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 76002/76002 [00:07<00:00, 10097.37 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest checkpoint found: /mnt/c/Users/user/DD_Pipeline/model_training/smiles_gpt2/checkpoint-4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='16152' max='16152' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [16152/16152 2:27:43, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.047000</td>\n",
       "      <td>0.043046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.043000</td>\n",
       "      <td>0.041002</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed on file /mnt/c/Users/user/DD_Pipeline/Training_SMILES/chunk5.csv. Checkpoint saved at /mnt/c/Users/user/DD_Pipeline/model_training/smiles_gpt2/checkpoint-5.\n",
      "Waiting for 30 minutes before training on the next chunk...\n",
      "Processing file: /mnt/c/Users/user/DD_Pipeline/Training_SMILES/chunk6.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68798/68798 [00:06<00:00, 9971.07 examples/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest checkpoint found: /mnt/c/Users/user/DD_Pipeline/model_training/smiles_gpt2/checkpoint-5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='14620' max='14620' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [14620/14620 2:13:43, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.045200</td>\n",
       "      <td>0.044249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.042200</td>\n",
       "      <td>0.041948</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed on file /mnt/c/Users/user/DD_Pipeline/Training_SMILES/chunk6.csv. Checkpoint saved at /mnt/c/Users/user/DD_Pipeline/model_training/smiles_gpt2/checkpoint-6.\n",
      "Waiting for 30 minutes before training on the next chunk...\n",
      "Processing file: /mnt/c/Users/user/DD_Pipeline/Training_SMILES/chunk7.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 73072/73072 [00:07<00:00, 10071.21 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest checkpoint found: /mnt/c/Users/user/DD_Pipeline/model_training/smiles_gpt2/checkpoint-6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15528' max='15528' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15528/15528 2:22:14, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.041800</td>\n",
       "      <td>0.041603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.040600</td>\n",
       "      <td>0.039574</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed on file /mnt/c/Users/user/DD_Pipeline/Training_SMILES/chunk7.csv. Checkpoint saved at /mnt/c/Users/user/DD_Pipeline/model_training/smiles_gpt2/checkpoint-7.\n",
      "Waiting for 30 minutes before training on the next chunk...\n",
      "Processing file: /mnt/c/Users/user/DD_Pipeline/Training_SMILES/chunk8.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 65903/65903 [00:06<00:00, 9852.18 examples/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest checkpoint found: /mnt/c/Users/user/DD_Pipeline/model_training/smiles_gpt2/checkpoint-7\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import warnings\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Suppress all warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def get_latest_checkpoint(base_dir):\n",
    "    \"\"\" Retrieve the most recent checkpoint from the base directory. \"\"\"\n",
    "    try:\n",
    "        checkpoints = [os.path.join(base_dir, d) for d in os.listdir(base_dir) if d.startswith('checkpoint-')]\n",
    "        if checkpoints:\n",
    "            latest_checkpoint = max(checkpoints, key=os.path.getmtime)\n",
    "            print(f\"Latest checkpoint found: {latest_checkpoint}\")\n",
    "            return latest_checkpoint\n",
    "        else:\n",
    "            print(\"No checkpoints found, will start from base model.\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error finding latest checkpoint: {e}\")\n",
    "        return None\n",
    "\n",
    "# Directory containing your CSV files\n",
    "data_dir = \"/mnt/c/Users/user/DD_Pipeline/Training_SMILES\"\n",
    "# Base directory where checkpoints are saved\n",
    "base_checkpoint_dir = \"/mnt/c/Users/user/DD_Pipeline/model_training/smiles_gpt2\"\n",
    "\n",
    "# Ensure the log directory exists\n",
    "log_dir = './training_logs'\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Set the eos_token as padding token\n",
    "\n",
    "def load_and_tokenize_chunk(file_path):\n",
    "    try:\n",
    "        dataset = load_dataset('csv', data_files=file_path)\n",
    "        def tokenize_function(examples):\n",
    "            tokenized = tokenizer(examples['SMILES'], truncation=True, padding='max_length', max_length=500)\n",
    "            tokenized['labels'] = tokenized['input_ids'].copy()\n",
    "            return tokenized\n",
    "        tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "        split_dataset = tokenized_dataset['train'].train_test_split(test_size=0.15)\n",
    "        return split_dataset\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def log_losses(log_file_path, train_loss, val_loss=None):\n",
    "    with open(log_file_path, 'a') as log_file:\n",
    "        log_file.write(f\"Training loss: {train_loss}\\n\")\n",
    "        if val_loss:\n",
    "            log_file.write(f\"Validation loss: {val_loss}\\n\")\n",
    "\n",
    "# Get all CSV files and sort them in the correct numerical order of chunks\n",
    "try:\n",
    "    csv_files = sorted([os.path.join(data_dir, f) for f in os.listdir(data_dir) if f.endswith('.csv')],\n",
    "                       key=lambda x: int(x.split('chunk')[-1].split('.')[0]))\n",
    "except Exception as e:\n",
    "    print(f\"Error sorting CSV files: {e}\")\n",
    "    csv_files = []\n",
    "\n",
    "# Start training from chunk1.csv\n",
    "for i, file_path in enumerate(csv_files, start=1):\n",
    "    print(f\"Processing file: {file_path}\")\n",
    "    split_dataset = load_and_tokenize_chunk(file_path)\n",
    "    if split_dataset is None:\n",
    "        continue\n",
    "\n",
    "    # Dynamically load the latest checkpoint for each new training session\n",
    "    latest_checkpoint_path = get_latest_checkpoint(base_checkpoint_dir)\n",
    "    if latest_checkpoint_path:\n",
    "        model = GPT2LMHeadModel.from_pretrained(latest_checkpoint_path)\n",
    "    else:\n",
    "        model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=base_checkpoint_dir,\n",
    "        eval_strategy=\"epoch\",\n",
    "        learning_rate=5e-5,\n",
    "        weight_decay=0.01,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        num_train_epochs=2,\n",
    "        save_total_limit=2,\n",
    "        resume_from_checkpoint=latest_checkpoint_path,\n",
    "        logging_dir=\"./logs\",\n",
    "        logging_steps=10,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=split_dataset['train'],\n",
    "        eval_dataset=split_dataset['test'],\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        train_output = trainer.train()\n",
    "        new_checkpoint_dir = os.path.join(base_checkpoint_dir, f\"checkpoint-{i}\")\n",
    "        model.save_pretrained(new_checkpoint_dir)\n",
    "        train_loss = train_output.training_loss\n",
    "        log_file_path = os.path.join(log_dir, f\"chunk_{i}_training_log.txt\")\n",
    "        log_losses(log_file_path, train_loss)\n",
    "        print(f\"Training completed on file {file_path}. Checkpoint saved at {new_checkpoint_dir}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Training failed for {file_path} due to {e}\")\n",
    "    finally:\n",
    "        print(\"Waiting for 30 minutes before training on the next chunk...\")\n",
    "        time.sleep(1800)  # 10 minutes pause\n",
    "\n",
    "print(\"Training completed on all files!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/user/DD_Pipeline/env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-09-10 16:47:18.936512: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-09-10 16:47:19.916446: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-09-10 16:47:20.266810: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-09-10 16:47:20.363238: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-09-10 16:47:21.091445: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-10 16:47:28.477530: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: /mnt/c/Users/user/DD_Pipeline/Training_SMILES/chunk1.csv\n",
      "No checkpoints found, will start from base model.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13554' max='13554' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13554/13554 2:05:48, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.062700</td>\n",
       "      <td>0.055595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.051600</td>\n",
       "      <td>0.050487</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed on file /mnt/c/Users/user/DD_Pipeline/Training_SMILES/chunk1.csv. Checkpoint saved at /mnt/c/Users/user/DD_Pipeline/model_training/smiles_gpt2/checkpoint-1.\n",
      "Waiting for 30 minutes before training on the next chunk...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import warnings\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Suppress all warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def get_latest_checkpoint(base_dir):\n",
    "    \"\"\" Retrieve the most recent checkpoint from the base directory. \"\"\"\n",
    "    try:\n",
    "        checkpoints = [os.path.join(base_dir, d) for d in os.listdir(base_dir) if d.startswith('checkpoint-')]\n",
    "        if checkpoints:\n",
    "            latest_checkpoint = max(checkpoints, key=os.path.getmtime)\n",
    "            print(f\"Latest checkpoint found: {latest_checkpoint}\")\n",
    "            return latest_checkpoint\n",
    "        else:\n",
    "            print(\"No checkpoints found, will start from base model.\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error finding latest checkpoint: {e}\")\n",
    "        return None\n",
    "\n",
    "# Directory containing your CSV files\n",
    "data_dir = \"/mnt/c/Users/user/DD_Pipeline/Training_SMILES\"\n",
    "# Base directory where checkpoints are saved\n",
    "base_checkpoint_dir = \"/mnt/c/Users/user/DD_Pipeline/model_training/smiles_gpt2\"\n",
    "\n",
    "# Ensure the log directory exists\n",
    "log_dir = './training_logs'\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Set the eos_token as padding token\n",
    "\n",
    "def load_and_tokenize_chunk(file_path):\n",
    "    try:\n",
    "        dataset = load_dataset('csv', data_files=file_path)\n",
    "        def tokenize_function(examples):\n",
    "            tokenized = tokenizer(examples['SMILES'], truncation=True, padding='max_length', max_length=500)\n",
    "            tokenized['labels'] = tokenized['input_ids'].copy()\n",
    "            return tokenized\n",
    "        tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "        split_dataset = tokenized_dataset['train'].train_test_split(test_size=0.15)\n",
    "        return split_dataset\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def log_losses(log_file_path, train_loss, val_loss=None):\n",
    "    with open(log_file_path, 'a') as log_file:\n",
    "        log_file.write(f\"Training loss: {train_loss}\\n\")\n",
    "        if val_loss:\n",
    "            log_file.write(f\"Validation loss: {val_loss}\\n\")\n",
    "\n",
    "# Get all CSV files and sort them in the correct numerical order of chunks\n",
    "try:\n",
    "    csv_files = sorted([os.path.join(data_dir, f) for f in os.listdir(data_dir) if f.endswith('.csv')],\n",
    "                       key=lambda x: int(x.split('chunk')[-1].split('.')[0]))\n",
    "except Exception as e:\n",
    "    print(f\"Error sorting CSV files: {e}\")\n",
    "    csv_files = []\n",
    "\n",
    "# Start training from chunk1.csv\n",
    "for i, file_path in enumerate(csv_files, start=1):\n",
    "    print(f\"Processing file: {file_path}\")\n",
    "    split_dataset = load_and_tokenize_chunk(file_path)\n",
    "    if split_dataset is None:\n",
    "        continue\n",
    "\n",
    "    # Dynamically load the latest checkpoint for each new training session\n",
    "    latest_checkpoint_path = get_latest_checkpoint(base_checkpoint_dir)\n",
    "    if latest_checkpoint_path:\n",
    "        model = GPT2LMHeadModel.from_pretrained(latest_checkpoint_path)\n",
    "    else:\n",
    "        model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=base_checkpoint_dir,\n",
    "        eval_strategy=\"epoch\",\n",
    "        learning_rate=5e-5,\n",
    "        weight_decay=0.01,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        num_train_epochs=2,\n",
    "        save_total_limit=2,\n",
    "        resume_from_checkpoint=latest_checkpoint_path,\n",
    "        logging_dir=\"./logs\",\n",
    "        logging_steps=10,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=split_dataset['train'],\n",
    "        eval_dataset=split_dataset['test'],\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        train_output = trainer.train()\n",
    "        new_checkpoint_dir = os.path.join(base_checkpoint_dir, f\"checkpoint-{i}\")\n",
    "        model.save_pretrained(new_checkpoint_dir)\n",
    "        train_loss = train_output.training_loss\n",
    "        log_file_path = os.path.join(log_dir, f\"chunk_{i}_training_log.txt\")\n",
    "        log_losses(log_file_path, train_loss)\n",
    "        print(f\"Training completed on file {file_path}. Checkpoint saved at {new_checkpoint_dir}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Training failed for {file_path} due to {e}\")\n",
    "    finally:\n",
    "        print(\"Waiting for 30 minutes before training on the next chunk...\")\n",
    "        time.sleep(1800)  # 10 minutes pause\n",
    "\n",
    "print(\"Training completed on all files!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
